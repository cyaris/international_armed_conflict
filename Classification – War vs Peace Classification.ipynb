{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn.over_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "# from sklearn.metrics import roc_curve\n",
    "# from sklearn.metrics import auc\n",
    "# from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from notebook_pbar import * # import my notebook_pbar.py file\n",
    "# timelist = timelist # import the default variables timelist and then_time\n",
    "# then_time = then_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function cm_val creates an interactive confusion matrix on un-scaled data.\n",
    "# function cm_val_scaled creates an interactive confusion matrix on scaled data.\n",
    "from my_functions import cm_val\n",
    "from my_functions import cm_val_scaled\n",
    "# function y_pred_inverse extracts the predictive probability from predict_proba.\n",
    "from my_functions import y_pred_inverse\n",
    "# function plot_validation_curve_log plots a validation curve on a log scale.\n",
    "# function plot_validation_curve_reg plots a validation curve on a default scale.\n",
    "from my_functions import plot_validation_curve_log\n",
    "from my_functions import plot_validation_curve_reg\n",
    "# function plot_learning_curve_reg plots a learning curve on a default scale.\n",
    "from my_functions import plot_learning_curve\n",
    "# function plot_decision_tree uses graphviz to visualize the splits of a devision tree.\n",
    "from my_functions import plot_decision_tree\n",
    "# function train_and_calibrate_cv performs stratified shuffle split on a specified model,\n",
    "# returning validation scores and roc/auc.\n",
    "from my_functions import train_and_calibrate_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('pickle/df_modeling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9115"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['num_trade_states', 'export_dollars', 'import_dollars',\n",
       "       'prim_energy_consumption', 'total_pop', 'cinc_score', 'num_alliances',\n",
       "       'pre_1816_alliances', 'num_in_effect_1231_2012', 'defense_treaties',\n",
       "       'neutrality_treaties', 'nonaggression_treaties', 'entente_treaties',\n",
       "       'avg_cum_duration', 'ongoing_2010', 'revision_pct',\n",
       "       'num_leadership_changes', 'leader_tenure', 'age_govt',\n",
       "       'num_transitions_ever', 'Americas', 'Asia', 'Europe', 'Oceania',\n",
       "       'Indirect election', 'Nonelective', 'No legislature exists',\n",
       "       'Non‐elective legislature', 'Appointed', 'Closed', 'Elected',\n",
       "       'All parties legally banned', 'Legally single party state',\n",
       "       'Multiple parties legally allowed', 'Multiple parties', 'No parties',\n",
       "       'One party', 'Multiple parties outside regime',\n",
       "       'No parties outside regime', 'One party outside regime',\n",
       "       'Legislature with multiple parties',\n",
       "       'No legislature or all nonpartisan', 'Only members from regime party',\n",
       "       'Rules rewritten unconstitutionally', 'collective_leadership',\n",
       "       'military_leader', 'royal_leader', 'nominal_vs_eff_diff',\n",
       "       'communist_leader', 'leader_died', 'democratic_regime',\n",
       "       'cabinet_assembly', 'popular_election', 'Civilian dictatorship',\n",
       "       'Military dictatorship', 'Mixed (semi‐presidential) democracy',\n",
       "       'Parliamentary democracy', 'Presidential democracy',\n",
       "       'Royal dictatorship', 'transition_to_democracy',\n",
       "       'transition_to_dictatorship', 'war_present', '40s', '50s', '60s', '70s',\n",
       "       '80s', '90s'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['cow_code', 'year', 'state_name', 'export_import_ratio', 'avg_hostility_level', 'military_expenditure', 'military_personnel', 'num_wars', 'num_conflicts'], axis = 1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_trade_states                               1395140.000\n",
       "export_dollars                         158215503813141.281\n",
       "import_dollars                         158243361464238.688\n",
       "prim_energy_consumption               1176329088000000.000\n",
       "total_pop                                 266477614000.000\n",
       "cinc_score                                          63.399\n",
       "num_alliances                                   120407.000\n",
       "pre_1816_alliances                                 126.000\n",
       "num_in_effect_1231_2012                         102323.000\n",
       "defense_treaties                                 95462.000\n",
       "neutrality_treaties                               2086.000\n",
       "nonaggression_treaties                           90735.000\n",
       "entente_treaties                                103066.000\n",
       "ongoing_2010                                        19.850\n",
       "num_leadership_changes                            2040.000\n",
       "leader_tenure                                    68636.000\n",
       "age_govt                                        285851.000\n",
       "num_transitions_ever                              3160.000\n",
       "Americas                                          1844.000\n",
       "Asia                                              2329.000\n",
       "Europe                                            2016.000\n",
       "Oceania                                            499.000\n",
       "Indirect election                                 3833.000\n",
       "Nonelective                                       2479.000\n",
       "No legislature exists                              988.000\n",
       "Non‐elective legislature                           445.000\n",
       "Appointed                                          379.000\n",
       "Closed                                            1164.000\n",
       "Elected                                           7539.000\n",
       "All parties legally banned                         959.000\n",
       "                                              ...         \n",
       "Multiple parties outside regime                   6422.000\n",
       "No parties outside regime                          925.000\n",
       "One party outside regime                          1747.000\n",
       "Legislature with multiple parties                 5730.000\n",
       "No legislature or all nonpartisan                 1748.000\n",
       "Only members from regime party                    1604.000\n",
       "Rules rewritten unconstitutionally                 317.000\n",
       "collective_leadership                              123.000\n",
       "military_leader                                   1961.000\n",
       "royal_leader                                       778.000\n",
       "nominal_vs_eff_diff                                778.000\n",
       "communist_leader                                   694.000\n",
       "leader_died                                        139.000\n",
       "democratic_regime                                 3995.000\n",
       "cabinet_assembly                                  2940.000\n",
       "popular_election                                  1726.000\n",
       "Civilian dictatorship                             2649.000\n",
       "Military dictatorship                             1694.000\n",
       "Mixed (semi‐presidential) democracy                689.000\n",
       "Parliamentary democracy                           2191.000\n",
       "Presidential democracy                            1115.000\n",
       "Royal dictatorship                                 777.000\n",
       "transition_to_democracy                            107.000\n",
       "transition_to_dictatorship                          67.000\n",
       "40s                                                300.000\n",
       "50s                                                853.000\n",
       "60s                                               1235.000\n",
       "70s                                               1490.000\n",
       "80s                                               1651.000\n",
       "90s                                               1866.000\n",
       "Length: 65, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['cow_code', 'year', 'state_name', 'export_import_ratio', 'avg_hostility_level',\n",
    "             'military_expenditure', 'military_personnel', 'num_wars', 'num_conflicts',\n",
    "             'war_present', 'revision_pct', 'avg_cum_duration'], axis = 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['cow_code', 'year', 'state_name', 'export_import_ratio', 'avg_hostility_level',\n",
    "             'military_expenditure', 'military_personnel', 'num_wars', 'num_conflicts',\n",
    "             'war_present', 'revision_pct', 'avg_cum_duration', 'ongoing_2010'], axis = 1)\n",
    "\n",
    "y = df['war_present']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1001)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8871\n",
       "1     244\n",
       "Name: war_present, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['war_present'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_columns = ['num_trade_states', 'export_dollars', 'import_dollars',\n",
    "                      'prim_energy_consumption', 'total_pop', 'cinc_score',\n",
    "                      'num_alliances', 'pre_1816_alliances', 'num_in_effect_1231_2012',\n",
    "                      'defense_treaties', 'neutrality_treaties', 'nonaggression_treaties',\n",
    "                      'entente_treaties', 'leader_tenure',\n",
    "                      'age_govt', 'num_transitions_ever', 'num_leadership_changes']\n",
    "\n",
    "X_train_cont = X_train[continuous_columns]\n",
    "X_train_cont = X_train_cont.reset_index()\n",
    "X_train_cont.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "X_train_dummy = X_train.drop(continuous_columns, axis = 1)\n",
    "X_train_dummy = X_train_dummy.reset_index()\n",
    "X_train_dummy.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "# X_val_cont = X_val[continuous_columns]\n",
    "# X_val_cont = X_val_cont.reset_index()\n",
    "# X_val_cont.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "# X_val_dummy = X_val.drop(continuous_columns, axis = 1)\n",
    "# X_val_dummy = X_val_dummy.reset_index()\n",
    "# X_val_dummy.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "X_test_cont = X_test[continuous_columns]\n",
    "X_test_cont = X_test_cont.reset_index()\n",
    "X_test_cont.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "X_test_dummy = X_test.drop(continuous_columns, axis = 1)\n",
    "X_test_dummy = X_test_dummy.reset_index()\n",
    "X_test_dummy.drop(['index'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_cont_scaled = pd.DataFrame(scaler.fit_transform(X_train_cont))\n",
    "X_train_cont_scaled = X_train_cont_scaled.reset_index()\n",
    "X_train_cont_scaled.drop(['index'], axis = 1, inplace = True)\n",
    "X_train_cont_scaled.columns = continuous_columns\n",
    "\n",
    "# X_val_cont_scaled = pd.DataFrame(scaler.fit_transform(X_val_cont))\n",
    "# X_val_cont_scaled = X_val_cont_scaled.reset_index()\n",
    "# X_val_cont_scaled.drop(['index'], axis = 1, inplace = True)\n",
    "# X_val_cont_scaled.columns = continuous_columns\n",
    "\n",
    "X_test_cont_scaled = pd.DataFrame(scaler.transform(X_test_cont))\n",
    "X_test_cont_scaled = X_test_cont_scaled.reset_index()\n",
    "X_test_cont_scaled.drop(['index'], axis = 1, inplace = True)\n",
    "X_test_cont_scaled.columns = continuous_columns\n",
    "\n",
    "X_train_scaled = pd.concat([X_train_cont_scaled, X_train_dummy], axis = 1)\n",
    "# X_val_scaled = pd.concat([X_val_cont_scaled, X_val_dummy], axis = 1)\n",
    "X_test_scaled = pd.concat([X_test_cont_scaled, X_test_dummy], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7084\n",
       "1     208\n",
       "Name: war_present, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_train)['war_present'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(y_val)['war_present'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used smote to perform upsampling with added variance for all subsets of my data,\n",
    "# renaming them so that I still have the original versions that are not oversampled.\n",
    "smote = imblearn.over_sampling.SMOTE(ratio = {0: 7084, 1: (7084)}, random_state = 101)\n",
    "X_train_scaled, y_train = smote.fit_sample(X_train_scaled, y_train)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled)\n",
    "X_train_scaled.columns = X.columns\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_train.columns = ['war_present']\n",
    "\n",
    "# smote = imblearn.over_sampling.SMOTE(ratio = {0: 1775, 1: (1775)}, random_state = 101)\n",
    "# X_val_scaled, y_val = smote.fit_sample(X_val_scaled, y_val)\n",
    "\n",
    "# X_val_scaled = pd.DataFrame(X_val_scaled)\n",
    "# X_val_scaled.columns = X.columns\n",
    "# y_val = pd.DataFrame(y_val)\n",
    "# y_val.columns = ['war_present']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14168, 64)\n",
      "(1823, 64)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_scaled.shape)\n",
    "# print(X_val_scaled.shape)\n",
    "print(X_test_scaled.shape)\n",
    "print(y_train.shape)\n",
    "# print(y_val.shape)\n",
    "print(y_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.79      0.88      1787\n",
      "           1       0.05      0.56      0.09        36\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      1823\n",
      "   macro avg       0.52      0.67      0.49      1823\n",
      "weighted avg       0.97      0.79      0.86      1823\n",
      "\n",
      "[[1413  374]\n",
      " [  16   20]]\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty = 'l1')\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "predictions = lr.predict(X_test_scaled)\n",
    "print(metrics.classification_report(y_test, predictions))\n",
    "print(metrics.confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.79      0.88      1787\n",
      "           1       0.05      0.56      0.09        36\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      1823\n",
      "   macro avg       0.52      0.67      0.49      1823\n",
      "weighted avg       0.97      0.79      0.86      1823\n",
      "\n",
      "[[1412  375]\n",
      " [  16   20]]\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty = 'l2')\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "predictions = lr.predict(X_test_scaled)\n",
    "print(metrics.classification_report(y_test, predictions))\n",
    "print(metrics.confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbeta = make_scorer(fbeta_score, average = 'weighted', beta = 0.5)\n",
    "fbeta = make_scorer(fbeta_score, pos_label = 1, average = 'binary', beta = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=3, random_state=101, shuffle=True),\n",
       "          error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=101, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
       "          param_distributions={'n_estimators': [300], 'max_depth': array([6, 7, 8, 9]), 'max_features': array([3, 4, 5, 6]), 'min_samples_split': array([6]), 'min_samples_leaf': array([2, 3, 4, 5, 6]), 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=101, refit=True,\n",
       "          return_train_score='warn',\n",
       "          scoring=make_scorer(fbeta_score, pos_label=0, average=binary, beta=0.5),\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 101)\n",
    "param_grid = {'n_estimators': [300], 'max_depth': np.arange(6, 10), 'max_features': np.arange(3, 7), 'min_samples_split': np.arange(6, 7), 'min_samples_leaf': np.arange(2, 7), 'bootstrap': [True, False]}\n",
    "rand = RandomizedSearchCV(RandomForestClassifier(random_state = 101), param_distributions = param_grid, cv = cv, scoring = fbeta, refit = True, random_state = 101)\n",
    "rand.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9764037054035928\n",
      "0.9714053656301033\n",
      "\n",
      "{'n_estimators': 300, 'min_samples_split': 6, 'min_samples_leaf': 6, 'max_features': 3, 'max_depth': 9, 'bootstrap': False}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1787\n",
      "           1       0.00      0.00      0.00        36\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1823\n",
      "   macro avg       0.49      0.50      0.50      1823\n",
      "weighted avg       0.96      0.98      0.97      1823\n",
      "\n",
      "[[1787    0]\n",
      " [  36    0]]\n"
     ]
    }
   ],
   "source": [
    "predictions = rand.predict(X_test)\n",
    "print(rand.cv_results_['mean_train_score'].mean())\n",
    "print(rand.cv_results_['mean_test_score'].mean())\n",
    "print('')\n",
    "print(rand.best_params_)\n",
    "print(metrics.classification_report(y_test, predictions))\n",
    "print(metrics.confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0. 0. 0. ... 0. 0. 0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-411f1b348312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# is this formula for adjusted_r_squared correct?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mr_squared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0madjusted_r_squared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mr_squared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_squared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjusted_r_squared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/lpthw/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    460\u001b[0m                              % self.best_estimator_)\n\u001b[1;32m    461\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultimetric_\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/lpthw/lib/python3.6/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \"\"\"\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n",
      "\u001b[0;32m~/.venvs/lpthw/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \"\"\"\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/lpthw/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimators_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/lpthw/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    362\u001b[0m                                  \"call `fit` before exploiting the model.\")\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/lpthw/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;34m\"\"\"Validate X whenever one tries to predict, apply, predict_proba\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n\u001b[1;32m    378\u001b[0m                                 X.indptr.dtype != np.intc):\n",
      "\u001b[0;32m~/.venvs/lpthw/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    545\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0. 0. 0. ... 0. 0. 0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# is this formula for adjusted_r_squared correct?\n",
    "r_squared = rand.score(y_test, predictions)\n",
    "adjusted_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-X.shape[1]-1)\n",
    "print(r_squared, adjusted_r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created new confusion matrix for tuned model.\n",
    "print('\\n', metrics.classification_report(y_test, predictions))\n",
    "\n",
    "df_cm = pd.DataFrame(metrics.confusion_matrix(y_test, predictions))\n",
    "df_cm.rename({0: 'Peace', 1: 'War'}, axis = 1, inplace = True)\n",
    "df_cm.rename(index = {0: 'Peace', 1: 'War'}, inplace = True)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (5, 5))\n",
    "sns.set_context(font_scale = 1.2)\n",
    "sns.heatmap(df_cm, annot = True, fmt = 'g', cbar = False, cmap = 'cividis')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_xlabel('Predicted Label');\n",
    "ax.xaxis.set_ticks_position('top') \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 101)\n",
    "\n",
    "params_grid = {\n",
    "    'max_depth': [6],\n",
    "    'n_estimators': [50, 100, 200, 300, 400],\n",
    "    'learning_rate': np.linspace(0.5, 1, 2, 3)\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    'objective':'binary:logistic',\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "best_grid = GridSearchCV(\n",
    "    estimator = XGBClassifier(**params_fixed, seed = 42),\n",
    "    param_grid = params_grid,\n",
    "    cv = cv,\n",
    "    scoring = 'roc_auc'\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best accuracy obtained {0}\".format(best_grid.best_score_))\n",
    "print(\"Parameters:\")\n",
    "for key, value in best_grid.best_params_.items():\n",
    "    print(\"\\t{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created new confusion matrix for tuned model.\n",
    "print('\\n', metrics.classification_report(y_test, predictions))\n",
    "\n",
    "df_cm = pd.DataFrame(metrics.confusion_matrix(y_test, predictions))\n",
    "df_cm.rename({0: 'Peace', 1: 'War'}, axis = 1, inplace = True)\n",
    "df_cm.rename(index = {0: 'Peace', 1: 'War'}, inplace = True)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (5, 5))\n",
    "sns.set_context(font_scale = 1.2)\n",
    "sns.heatmap(df_cm, annot = True, fmt = 'g', cbar = False, cmap = 'cividis')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_xlabel('Predicted Label');\n",
    "ax.xaxis.set_ticks_position('top') \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
